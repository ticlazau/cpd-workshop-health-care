{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Analyzing Health Care data with Cloud Pak for Data on OpenShift \u00b6 Welcome to our workshop! In this workshop we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications. The goals of this workshop are: Collect and virtualize data Visualize data with Data Refinery Create and deploy a machine learning model Monitor the model Create a Python app to use the model About this workshop \u00b6 The introductory page of the workshop is broken down into the following sections: Agenda Compatability Credits Agenda \u00b6 00:20 Lecture - Intro and Overview Introduction to Cloud Pak for Data and an Overview of this workshop 00:20 Lab - Pre-work Creating a project, downloading the data set, seeding a database 00:30 Lab - [Pre-work buffer] Extra time to solve initial problems 00:20 Lecture - Data Refinery and Data Virtualization Data Refinery and Data Virtualization 00:10 Walkthrough - Data Connection and Virtualization Creating a new connection, virtualizing the data, importing the data into the project 00:30 Lab - Data Connection and Virtualization Creating a new connection, virtualizing the data, importing the data into the project 00:10 Lab - Import Data to Project Import the data into your project 00:10 Walkthrough - Data Visualization with Data Refinery Refining the data, vizualizing and profiling the data 00:15 Lab - Data Visualization with Data Refinery Refining the data, vizualizing and profiling the data 00:25 Lecture - Machine Learning Machine Learning and Deep Learning concepts 00:40 Lab - Machine Learning with Jupyter Building a model with Spark, deploying the model with Watson Maching Learning, testing the model with a Python Flask app 00:10 Walkthrough - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:20 Lab - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:15 Lecture - Monitor Models with Watson OpenScale Monitor for Bias, Fairness, Drift, and Quality 00:10 Walkthrough - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup 00:15 Lab - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup Compatability \u00b6 This workshop has been tested on the following platforms: macOS : Mojave (10.14), Catalina (10.15) Credits \u00b6 Scott D'Angelo Omid Meh Javier Torres","title":"About the workshop"},{"location":"#analyzing-health-care-data-with-cloud-pak-for-data-on-openshift","text":"Welcome to our workshop! In this workshop we'll be using the Cloud Pak for Data platform to Collect Data, Organize Data, Analyze Data, and Infuse AI into our applications. The goals of this workshop are: Collect and virtualize data Visualize data with Data Refinery Create and deploy a machine learning model Monitor the model Create a Python app to use the model","title":"Analyzing Health Care data with Cloud Pak for Data on OpenShift"},{"location":"#about-this-workshop","text":"The introductory page of the workshop is broken down into the following sections: Agenda Compatability Credits","title":"About this workshop"},{"location":"#agenda","text":"00:20 Lecture - Intro and Overview Introduction to Cloud Pak for Data and an Overview of this workshop 00:20 Lab - Pre-work Creating a project, downloading the data set, seeding a database 00:30 Lab - [Pre-work buffer] Extra time to solve initial problems 00:20 Lecture - Data Refinery and Data Virtualization Data Refinery and Data Virtualization 00:10 Walkthrough - Data Connection and Virtualization Creating a new connection, virtualizing the data, importing the data into the project 00:30 Lab - Data Connection and Virtualization Creating a new connection, virtualizing the data, importing the data into the project 00:10 Lab - Import Data to Project Import the data into your project 00:10 Walkthrough - Data Visualization with Data Refinery Refining the data, vizualizing and profiling the data 00:15 Lab - Data Visualization with Data Refinery Refining the data, vizualizing and profiling the data 00:25 Lecture - Machine Learning Machine Learning and Deep Learning concepts 00:40 Lab - Machine Learning with Jupyter Building a model with Spark, deploying the model with Watson Maching Learning, testing the model with a Python Flask app 00:10 Walkthrough - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:20 Lab - Machine Learning with AutoAI Use AutoAi to quickly generate a Machine Learning pipeline and model 00:15 Lecture - Monitor Models with Watson OpenScale Monitor for Bias, Fairness, Drift, and Quality 00:10 Walkthrough - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup 00:15 Lab - Monitoring models with OpenScale GUI (Auto setup Monitoring) Quickly deploy an OpenScale demo with Auto setup","title":"Agenda"},{"location":"#compatability","text":"This workshop has been tested on the following platforms: macOS : Mojave (10.14), Catalina (10.15)","title":"Compatability"},{"location":"#credits","text":"Scott D'Angelo Omid Meh Javier Torres","title":"Credits"},{"location":"SUMMARY/","text":"Summary \u00b6 Getting Started \u00b6 Pre-work Health Care Workshop \u00b6 Data Connection and Virtualization Data Visualization with Data Refinery Enterprise data governance for Admins using Watson Knowledge Catalog Machine Learning with AutoAI Monitoring models with OpenScale GUI (Auto setup Monitoring) Workshop Resources \u00b6 Instructor Guide Resources \u00b6 IBM Cloud Pak for Data Platform API IBM Cloud Pak for Data APIs Watson Knowledge Catalog Watson Knowledge Catalog Learning Center IBM Developer Cloud Pak Experiences - Free 7 day trial Cloud Pak for Applications","title":"Summary"},{"location":"SUMMARY/#summary","text":"","title":"Summary"},{"location":"SUMMARY/#getting-started","text":"Pre-work","title":"Getting Started"},{"location":"SUMMARY/#health-care-workshop","text":"Data Connection and Virtualization Data Visualization with Data Refinery Enterprise data governance for Admins using Watson Knowledge Catalog Machine Learning with AutoAI Monitoring models with OpenScale GUI (Auto setup Monitoring)","title":"Health Care Workshop"},{"location":"SUMMARY/#workshop-resources","text":"Instructor Guide","title":"Workshop Resources"},{"location":"SUMMARY/#resources","text":"IBM Cloud Pak for Data Platform API IBM Cloud Pak for Data APIs Watson Knowledge Catalog Watson Knowledge Catalog Learning Center IBM Developer Cloud Pak Experiences - Free 7 day trial Cloud Pak for Applications","title":"Resources"},{"location":"data-connection-and-virtualization/","text":"Data Connection and Virtualization \u00b6 This section will cover aspects of collecting data in Cloud Pak for Data. Specifically we will be connecting to different data sources and creating views against those data sources to create a single unified set of data assets that can be used in other modules of this workshop. The user persona involved is a Data Steward. The data can exist on any database, on premise or on the cloud. Using IBM Data Virtualization we can access the data without moving it, using specifically crafted SQL queries to view the data, join the data and perform other operations. The sequence we will follow will be: Note: To complete this section, an Admin or Data Engineer role needs to be assigned to your user account. The workshop instructor will assign this role as appropriate. The section is broken up into the following steps: Create Virtualized Tables Create Joined Virtual Views Grant Access to Virtualized Data Virtualizing Data \u00b6 In this section, we will gather data from several tables across data sources. We will use data virtualization to access these tables and then create joined views against those virtualized tables. Create Virtualized Tables \u00b6 To launch the data virtualization tool, go the (\u2630) navigation menu and click Data -> Data virtualization . From the Data virtualization sub-menu, Click on the Menu drop down list and choose Virtualization -> Virtualize . Several tables names will be displayed across any of the data sources that are included in the data virtualization server. You will notice that on the right panel, we can filter the tables being displayed by selecting the datasource. To simplify the search for tables you will use, click on the Schemas column header to sort the tables by Schema. Then find the tables we will be using for this workshop: MEDICATIONS , PATIENTS and CONDITIONS , which are under the CP4DHEALTH schema. Select the checkboxes next to these three tables, and then click on Add to cart followed by the View Cart button. Note: You may need to page through the available tables by clicking on the right arrow at the bottom of the tables view. The next panel prompts you to select where to assign the virtualized tables. Select the My virtualized data radio button. Click the Virtualize button to add the virtualized tables to your data (we left the default values, so the tables will be virtualized under your own user schema with the same table names as the original tables). You'll be notified that the virtual tables have been created. Let's see the new virtualized tables from the Data Virtualization tool by clicking View my virtualized data button. Create Joined Virtual Views \u00b6 Now we're going to join the tables we previously virtualized, so we have a final merged set of data. It will be easier to do it here rather than in a notebook where we'd have to write code to handle three different data sets. From the 'My virtualized data' page, Click on two of the virtualized tables ( PATIENTS and MEDICATIONS ) and click the Join button. To join the tables we need to pick a key that is common to both data sets. Here we choose to map ID from the PATIENTS table to PATIENT on the MEDICATIONS table. Do this by clicking on one and dragging it to another. When the line is drawn click on the Next button. In the next panel we will accept the existing names for our columns. Click the Next button to continue. In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose something like: XXXPATIENTMEDICATIONS (where XXX is my All Upper Case user ID or intitials). Also select the My virtualized data radio button. Click the Create view button to add the virtualized aggregate view to your data. You'll be notified that the join view creation has succeeded! Click on View my virutalized data button. Repeat the same steps as above, but this time choose to join the new joined view you just created ( XXXPATIENTMEDICATIONS ) and the last virtualized table ( CONDITIONS ), to create a new joined view that has all three tables. Click the Join button. Again join the two tables by selecting/mapping the ID from the XXXPATIENTMEDICATIONS table to PATIENT on the CONDITIONS . Do this by clicking on one and dragging it to another. When the line is drawn click on the Next button. In the next panel we can scroll to the right and see that there are duplicate columns for START,STOP,PATIENT,ENCOUNTER,CODE,DESCRIPTION . Rename them by appending CONDITION to each in order to avoid a naming conflict. Click the Next button to continue. In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose something like: XXXPATIENTMEDICATIONSCONDITIONS (where XXX is my All Upper Case user ID or intitials). Also select the My virtualized data radio button. If there is a Submit to catalog checkbox on the top right, unselect it and finally click the Create view button to add the virtualized aggregate view to your data. You'll be notified that the join view creation has succeeded! Click on View my virtualized data button. From the My virtualized data page you should now see all three virtualized tables and two joined tables. Next we'll assign our virtual data to our project. Select the Table box, or all boxes individually, and click Assign . For Assign to choose Project , then select the project that you created in the pre-work . Make sure that Submit to catalog is unchecked and click Assign . Go back to your virtualized data if you will complete the next section or finish with the Conclusion 2. Grant access to virtualized data \u00b6 Note: This section only needs to be completed if there are non-Admin or non-Data Engineer users you are working in a group with. The instructors would have indicated that it needs to be completed to give those users access to the data you have virtualized above. In order for other users to have access to the data that you just virtualized, you need to grant them access. Follow these steps to make your Virtualized data visible to them. If you are not already in the data virtualization tool, launch the tool by going to the (\u2630) navigation menu and click Data -> Data virtualization . From the Data virtualization sub-menu, Click on the Menu drop down list and choose Virtualization -> My virtualized data . For one of the virtualized data assets you've created, click the 3 vertical dots on the right ( Note: you will have to hover over the area all the way on the right of the table row to see the dots. ) and choose Manage access . Click the Specific users button and click the Add user button. Select the user (or multiple users) you wish to grant access to and click the Add users button. Repeat the above steps to give access to the remaining virtualized tables and views (all five that you created). Conclusion \u00b6 In this section we learned how to make connection to databases that contain our data, how to virtualize them, and how to allow other to collaborate with us and use the virtualized data. Remember that you can add data from different databases and servers if you need to. Moreover, you can virtualized these data from different sources together as well! The goal is to take care of bringing the data to the platform early on so all the data scientists can use it without reinventing the wheel while you keep full control of who has access to what data.","title":"Data Connetion and Virtualization"},{"location":"data-connection-and-virtualization/#data-connection-and-virtualization","text":"This section will cover aspects of collecting data in Cloud Pak for Data. Specifically we will be connecting to different data sources and creating views against those data sources to create a single unified set of data assets that can be used in other modules of this workshop. The user persona involved is a Data Steward. The data can exist on any database, on premise or on the cloud. Using IBM Data Virtualization we can access the data without moving it, using specifically crafted SQL queries to view the data, join the data and perform other operations. The sequence we will follow will be: Note: To complete this section, an Admin or Data Engineer role needs to be assigned to your user account. The workshop instructor will assign this role as appropriate. The section is broken up into the following steps: Create Virtualized Tables Create Joined Virtual Views Grant Access to Virtualized Data","title":"Data Connection and Virtualization"},{"location":"data-connection-and-virtualization/#virtualizing-data","text":"In this section, we will gather data from several tables across data sources. We will use data virtualization to access these tables and then create joined views against those virtualized tables.","title":"Virtualizing Data"},{"location":"data-connection-and-virtualization/#create-virtualized-tables","text":"To launch the data virtualization tool, go the (\u2630) navigation menu and click Data -> Data virtualization . From the Data virtualization sub-menu, Click on the Menu drop down list and choose Virtualization -> Virtualize . Several tables names will be displayed across any of the data sources that are included in the data virtualization server. You will notice that on the right panel, we can filter the tables being displayed by selecting the datasource. To simplify the search for tables you will use, click on the Schemas column header to sort the tables by Schema. Then find the tables we will be using for this workshop: MEDICATIONS , PATIENTS and CONDITIONS , which are under the CP4DHEALTH schema. Select the checkboxes next to these three tables, and then click on Add to cart followed by the View Cart button. Note: You may need to page through the available tables by clicking on the right arrow at the bottom of the tables view. The next panel prompts you to select where to assign the virtualized tables. Select the My virtualized data radio button. Click the Virtualize button to add the virtualized tables to your data (we left the default values, so the tables will be virtualized under your own user schema with the same table names as the original tables). You'll be notified that the virtual tables have been created. Let's see the new virtualized tables from the Data Virtualization tool by clicking View my virtualized data button.","title":"Create Virtualized Tables"},{"location":"data-connection-and-virtualization/#create-joined-virtual-views","text":"Now we're going to join the tables we previously virtualized, so we have a final merged set of data. It will be easier to do it here rather than in a notebook where we'd have to write code to handle three different data sets. From the 'My virtualized data' page, Click on two of the virtualized tables ( PATIENTS and MEDICATIONS ) and click the Join button. To join the tables we need to pick a key that is common to both data sets. Here we choose to map ID from the PATIENTS table to PATIENT on the MEDICATIONS table. Do this by clicking on one and dragging it to another. When the line is drawn click on the Next button. In the next panel we will accept the existing names for our columns. Click the Next button to continue. In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose something like: XXXPATIENTMEDICATIONS (where XXX is my All Upper Case user ID or intitials). Also select the My virtualized data radio button. Click the Create view button to add the virtualized aggregate view to your data. You'll be notified that the join view creation has succeeded! Click on View my virutalized data button. Repeat the same steps as above, but this time choose to join the new joined view you just created ( XXXPATIENTMEDICATIONS ) and the last virtualized table ( CONDITIONS ), to create a new joined view that has all three tables. Click the Join button. Again join the two tables by selecting/mapping the ID from the XXXPATIENTMEDICATIONS table to PATIENT on the CONDITIONS . Do this by clicking on one and dragging it to another. When the line is drawn click on the Next button. In the next panel we can scroll to the right and see that there are duplicate columns for START,STOP,PATIENT,ENCOUNTER,CODE,DESCRIPTION . Rename them by appending CONDITION to each in order to avoid a naming conflict. Click the Next button to continue. In the next panel we'll give our joined data view a unique name (to be consistent with SQL standards, pick an all uppercase name), choose something like: XXXPATIENTMEDICATIONSCONDITIONS (where XXX is my All Upper Case user ID or intitials). Also select the My virtualized data radio button. If there is a Submit to catalog checkbox on the top right, unselect it and finally click the Create view button to add the virtualized aggregate view to your data. You'll be notified that the join view creation has succeeded! Click on View my virtualized data button. From the My virtualized data page you should now see all three virtualized tables and two joined tables. Next we'll assign our virtual data to our project. Select the Table box, or all boxes individually, and click Assign . For Assign to choose Project , then select the project that you created in the pre-work . Make sure that Submit to catalog is unchecked and click Assign . Go back to your virtualized data if you will complete the next section or finish with the Conclusion","title":"Create Joined Virtual Views"},{"location":"data-connection-and-virtualization/#2-grant-access-to-virtualized-data","text":"Note: This section only needs to be completed if there are non-Admin or non-Data Engineer users you are working in a group with. The instructors would have indicated that it needs to be completed to give those users access to the data you have virtualized above. In order for other users to have access to the data that you just virtualized, you need to grant them access. Follow these steps to make your Virtualized data visible to them. If you are not already in the data virtualization tool, launch the tool by going to the (\u2630) navigation menu and click Data -> Data virtualization . From the Data virtualization sub-menu, Click on the Menu drop down list and choose Virtualization -> My virtualized data . For one of the virtualized data assets you've created, click the 3 vertical dots on the right ( Note: you will have to hover over the area all the way on the right of the table row to see the dots. ) and choose Manage access . Click the Specific users button and click the Add user button. Select the user (or multiple users) you wish to grant access to and click the Add users button. Repeat the above steps to give access to the remaining virtualized tables and views (all five that you created).","title":"2. Grant access to virtualized data"},{"location":"data-connection-and-virtualization/#conclusion","text":"In this section we learned how to make connection to databases that contain our data, how to virtualize them, and how to allow other to collaborate with us and use the virtualized data. Remember that you can add data from different databases and servers if you need to. Moreover, you can virtualized these data from different sources together as well! The goal is to take care of bringing the data to the platform early on so all the data scientists can use it without reinventing the wheel while you keep full control of who has access to what data.","title":"Conclusion"},{"location":"data-visualization-and-refinery/","text":"Data Visualization and Data Refinery \u00b6 Let's take a quick detour to the Data Refinery tool. Data Refinery can quickly filter and mutate data, create quick visualizations, and do other data cleansing tasks from an easy to use user interface. This section is broken up into the following steps: Load the data Refine the data Profile the data Visualize the data Note: The lab instructions below assume you have a project already and have data you will refine. If not, follow the instructions in the pre-work and import data to project sections to create a project and assign data to your project. 1. Load Data \u00b6 Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From the Project home, under the Assets tab, ensure the Data assets section is expanded or click on the arrow to toggle it and open up the list of data assets. Click the check box next to the merged data asset XXXPATIENTMEDICATIONSCONDITIONS (where XXX is your username or the username of the person who granted you data access) to check it, then click the 3 vertical dots to the right, and select the Refine option from the menu. Data Refinery will launch and open to the Data tab. It will also display the information panel with details of the data refinery flow and where the output of the flow will be placed. Go ahead and click the X to the right of the Information panel to close it. 2. Refine Data \u00b6 We'll start out in the Data tab where we wrangle, shape and refine our data. As you refine your data, IBM Data Refinery keeps track of the steps in your data flow. You can modify them and even select a step to return to a particular moment in your data\u2019s transformation. Create Transformation Flow \u00b6 With Data Refinery, we can transform our data by directly entering operations in R-style syntax or interactively by selecting operations from the menu. For example, start typing filter on the Command line and observe that autocomplete will give hints on the syntax and how to use the command. A filter operation syntax will be displayed in the Command line. Clicking on the operation name within the Command line will give hints on the syntax and how to use the command. For instance the following command filters for Patients who live in Lancaster : filter ( `CITY` == 'Lancaster' ) Type or cut-n-paste the filter above in the command line and click Apply . Notice that the remaining rows have the City 'Lancaster'. Now, click on the counter-clockwise \"back\" arrow to remove the filter. Alternately, we can also remove the filter by clicking the trash icon for the Filter step in the Steps panel on the right. We will use the UI to explore and transform the data. Click the +Operation button. Let's use the Filter operation to check some values. Click on Filter in the left panel. Let's look at all the Patients who are no longer alive. Select the DEATHDATE column from the Column drop down list, and select Is not empty from the Operator drop down list, and then click the Apply button. Now, click on the counter-clockwise \"back\" arrow to remove the filter. Alternately, we can also remove the filter by clicking the trash icon for the Filter step in the Steps panel on the right. We can perform other filters. Click on the +Operation button again, and this time select Filter and then select TOTALCOST from the Column drop down list and Is greater than from the Operator drop down list, and finally enter 2000 under Value and Click the Apply button. Remove the filter using one of the methods described above. Let's say we've decide that there are columns that we don't want to leave in our dataset ( maybe because they might not be usefule features in our Machine Learning model, or because we don't want to make those data attributes accessible to others, or any other reason). We'll remove the PREFIX , FIRST , LAST , SUFFIX , and MAIDEN columns. For each columnn to be removed: Click the +Operation button, then select the Remove operation. Click the Change column selection option, choose the desired column to remove (i.e. PREFIX ), click Next and click Apply . The columns will be removed. Repeat for each of the above columns. At this point, you have a data transformation flow with 5 steps. As we saw in the last section, we keep track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the Steps button. The operations that you have performed on the data will be shown. You can modify these steps in real time and save for future use. Schedule Jobs \u00b6 Data Refinery allows you to run jobs at scheduled times, and save the output. In this way, you can regularly refine new data as it is updated. Click on the \"jobs\" icon and then Save and create job option from the menu. Give the job a name and optional description. Click Next . Click Next on the Configure tab. Note that you could change Select environment to a different one at this time, assuming that you had alternate environments in your project. Under the Schedule tab, click the Schedule to run slider. Click around and notice that you can set the Start on time, which has a \"calendar\" icon for choosing the date. The Repeat choice works in the same way.. We won't schedule a job at this time, so go ahead and click the Schedule to run slider, and it will switch to Schedule off . Click Next to move on. Finally, under the Review and create tab you can Edit the Details, Configuration, or Schedule in this Data Refinery job. Also, hover over the output name, which in this case is * SCOTTDA.SDAPATIENTMEDICATIONSCONDITIONS_shaped.csv* . Click the Create and Run button. The job will be listed as Status Running and then the Status will change to Completed . You can click Edit job to make changes to the job. Give the job a name and optional description. Click Next . Click Next on the Configure tab. Note that you could change Select environment to a different one at this time, assuming that you had alternate environments in your project. Under the Schedule tab, click the Schedule to run slider. Click around and notice that you can set the Start on time, which has a \"calendar\" icon for choosing the date. The Repeat choice works in the same way.. We won't schedule a job at this time, so go ahead and click the Schedule to run slider, and it will switch to Schedule off . Click Next to move on. Finally, under the Review and create tab you can Edit the Details, Configuration, or Schedule in this Data Refinery job. Also, hover over the output name, which in this case is * SCOTTDA.SDAPATIENTMEDICATIONSCONDITIONS_shaped.csv* . Click the Create and Run button. The job will be listed as Status Running and then the Status will change to Completed . You can click Edit job to make changes to the job. You can click Edit next to Scheduled to run . Notice that you can toggle the Schedule to run switch and choose a date and time to run this transformation as a job as well as if and how frequently it will repeat. We will not run this as a job, go ahead and click the Cancel button 3. Profile Data \u00b6 Go back to your project by clicking on the project name in the directory path. Return to the profile view by clicking the check box next to the merged data asset XXXPATIENTMEDICATIONSCONDITIONS (where XXX is your username or the username of the person who granted you data access) to check it, then click the 3 vertical dots to the right, and select the Refine option from the menu. Clicking on the Profile tab will bring up a view of several statistics and histograms for the attributes in your data. You can get insight into the data from the views and statistics: The distribution of birthdates is heavily skewed towards older dates. There are over 3x more married people than single people in this dataset. There are over 2x more people of Irish descent than any other ethnicity. Females outnumber males. 4. Visualize Data \u00b6 Let's do some visual exploration of our data using charts and graphs. We can accomplish this in Data Refinery interactively without coding. Choose the Visualizations tab to bring up an option to choose which columns to visualize. Under Columns to Visualize choose BIRTHDATE . Click +Add column and then TOTALCOST , then click the Visualize data button. We first see that the select chart is a Scatter plot. There is some clustering of higher total costs on the left side of the graph, indicating higher total costs for older people, as expected. Hovering over the outlier on the y-axis shows an individual with a very high total cost. We may wish to remove this individual from the dataset during the feature engineering phase of building a machine learning model, as it is likely to skew the results. Move the slider at the bottom of the Scatter plot to the right a bit to scroll past that one outlier individual and we can see a more even distribution. Scroll back and forth to explore, noticing how the maximum for TOTALCOST tends to go down as the BIRTHDATE gets more recent. Hover over any data point to bring up the details. Also note that we can add items like Primary title , Subtitle , and Footnote . Click on the Actions arrow and notice that you can perform tasks such as Download chart details , Download chart image , Field format or set Global visualization preferences . Click on the Global visualization preferences in the Actions menu. Click and explore the tabs for Titles , Tools , Theme . You can launch the Theme Builder to get creative with colors and themes, and save a default. Conclusion \u00b6 We've seen a small sampling of the power of Data Refinery on IBM Cloud Pak for Data. We saw how we can transform data using R code, at the command line, or using various Operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profiile the data, so see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot mapping BIRTHDATE vs. TOTALCOST.","title":"Data Visualization with Data Refinery"},{"location":"data-visualization-and-refinery/#data-visualization-and-data-refinery","text":"Let's take a quick detour to the Data Refinery tool. Data Refinery can quickly filter and mutate data, create quick visualizations, and do other data cleansing tasks from an easy to use user interface. This section is broken up into the following steps: Load the data Refine the data Profile the data Visualize the data Note: The lab instructions below assume you have a project already and have data you will refine. If not, follow the instructions in the pre-work and import data to project sections to create a project and assign data to your project.","title":"Data Visualization and Data Refinery"},{"location":"data-visualization-and-refinery/#1-load-data","text":"Go the (\u2630) navigation menu and click on the Projects link and then click on your analytics project. From the Project home, under the Assets tab, ensure the Data assets section is expanded or click on the arrow to toggle it and open up the list of data assets. Click the check box next to the merged data asset XXXPATIENTMEDICATIONSCONDITIONS (where XXX is your username or the username of the person who granted you data access) to check it, then click the 3 vertical dots to the right, and select the Refine option from the menu. Data Refinery will launch and open to the Data tab. It will also display the information panel with details of the data refinery flow and where the output of the flow will be placed. Go ahead and click the X to the right of the Information panel to close it.","title":"1. Load Data"},{"location":"data-visualization-and-refinery/#2-refine-data","text":"We'll start out in the Data tab where we wrangle, shape and refine our data. As you refine your data, IBM Data Refinery keeps track of the steps in your data flow. You can modify them and even select a step to return to a particular moment in your data\u2019s transformation.","title":"2. Refine Data"},{"location":"data-visualization-and-refinery/#create-transformation-flow","text":"With Data Refinery, we can transform our data by directly entering operations in R-style syntax or interactively by selecting operations from the menu. For example, start typing filter on the Command line and observe that autocomplete will give hints on the syntax and how to use the command. A filter operation syntax will be displayed in the Command line. Clicking on the operation name within the Command line will give hints on the syntax and how to use the command. For instance the following command filters for Patients who live in Lancaster : filter ( `CITY` == 'Lancaster' ) Type or cut-n-paste the filter above in the command line and click Apply . Notice that the remaining rows have the City 'Lancaster'. Now, click on the counter-clockwise \"back\" arrow to remove the filter. Alternately, we can also remove the filter by clicking the trash icon for the Filter step in the Steps panel on the right. We will use the UI to explore and transform the data. Click the +Operation button. Let's use the Filter operation to check some values. Click on Filter in the left panel. Let's look at all the Patients who are no longer alive. Select the DEATHDATE column from the Column drop down list, and select Is not empty from the Operator drop down list, and then click the Apply button. Now, click on the counter-clockwise \"back\" arrow to remove the filter. Alternately, we can also remove the filter by clicking the trash icon for the Filter step in the Steps panel on the right. We can perform other filters. Click on the +Operation button again, and this time select Filter and then select TOTALCOST from the Column drop down list and Is greater than from the Operator drop down list, and finally enter 2000 under Value and Click the Apply button. Remove the filter using one of the methods described above. Let's say we've decide that there are columns that we don't want to leave in our dataset ( maybe because they might not be usefule features in our Machine Learning model, or because we don't want to make those data attributes accessible to others, or any other reason). We'll remove the PREFIX , FIRST , LAST , SUFFIX , and MAIDEN columns. For each columnn to be removed: Click the +Operation button, then select the Remove operation. Click the Change column selection option, choose the desired column to remove (i.e. PREFIX ), click Next and click Apply . The columns will be removed. Repeat for each of the above columns. At this point, you have a data transformation flow with 5 steps. As we saw in the last section, we keep track of each of the steps and we can even undo (or redo) an action using the circular arrows. To see the steps in the data flow that you have performed, click the Steps button. The operations that you have performed on the data will be shown. You can modify these steps in real time and save for future use.","title":"Create Transformation Flow"},{"location":"data-visualization-and-refinery/#schedule-jobs","text":"Data Refinery allows you to run jobs at scheduled times, and save the output. In this way, you can regularly refine new data as it is updated. Click on the \"jobs\" icon and then Save and create job option from the menu. Give the job a name and optional description. Click Next . Click Next on the Configure tab. Note that you could change Select environment to a different one at this time, assuming that you had alternate environments in your project. Under the Schedule tab, click the Schedule to run slider. Click around and notice that you can set the Start on time, which has a \"calendar\" icon for choosing the date. The Repeat choice works in the same way.. We won't schedule a job at this time, so go ahead and click the Schedule to run slider, and it will switch to Schedule off . Click Next to move on. Finally, under the Review and create tab you can Edit the Details, Configuration, or Schedule in this Data Refinery job. Also, hover over the output name, which in this case is * SCOTTDA.SDAPATIENTMEDICATIONSCONDITIONS_shaped.csv* . Click the Create and Run button. The job will be listed as Status Running and then the Status will change to Completed . You can click Edit job to make changes to the job. Give the job a name and optional description. Click Next . Click Next on the Configure tab. Note that you could change Select environment to a different one at this time, assuming that you had alternate environments in your project. Under the Schedule tab, click the Schedule to run slider. Click around and notice that you can set the Start on time, which has a \"calendar\" icon for choosing the date. The Repeat choice works in the same way.. We won't schedule a job at this time, so go ahead and click the Schedule to run slider, and it will switch to Schedule off . Click Next to move on. Finally, under the Review and create tab you can Edit the Details, Configuration, or Schedule in this Data Refinery job. Also, hover over the output name, which in this case is * SCOTTDA.SDAPATIENTMEDICATIONSCONDITIONS_shaped.csv* . Click the Create and Run button. The job will be listed as Status Running and then the Status will change to Completed . You can click Edit job to make changes to the job. You can click Edit next to Scheduled to run . Notice that you can toggle the Schedule to run switch and choose a date and time to run this transformation as a job as well as if and how frequently it will repeat. We will not run this as a job, go ahead and click the Cancel button","title":"Schedule Jobs"},{"location":"data-visualization-and-refinery/#3-profile-data","text":"Go back to your project by clicking on the project name in the directory path. Return to the profile view by clicking the check box next to the merged data asset XXXPATIENTMEDICATIONSCONDITIONS (where XXX is your username or the username of the person who granted you data access) to check it, then click the 3 vertical dots to the right, and select the Refine option from the menu. Clicking on the Profile tab will bring up a view of several statistics and histograms for the attributes in your data. You can get insight into the data from the views and statistics: The distribution of birthdates is heavily skewed towards older dates. There are over 3x more married people than single people in this dataset. There are over 2x more people of Irish descent than any other ethnicity. Females outnumber males.","title":"3. Profile Data"},{"location":"data-visualization-and-refinery/#4-visualize-data","text":"Let's do some visual exploration of our data using charts and graphs. We can accomplish this in Data Refinery interactively without coding. Choose the Visualizations tab to bring up an option to choose which columns to visualize. Under Columns to Visualize choose BIRTHDATE . Click +Add column and then TOTALCOST , then click the Visualize data button. We first see that the select chart is a Scatter plot. There is some clustering of higher total costs on the left side of the graph, indicating higher total costs for older people, as expected. Hovering over the outlier on the y-axis shows an individual with a very high total cost. We may wish to remove this individual from the dataset during the feature engineering phase of building a machine learning model, as it is likely to skew the results. Move the slider at the bottom of the Scatter plot to the right a bit to scroll past that one outlier individual and we can see a more even distribution. Scroll back and forth to explore, noticing how the maximum for TOTALCOST tends to go down as the BIRTHDATE gets more recent. Hover over any data point to bring up the details. Also note that we can add items like Primary title , Subtitle , and Footnote . Click on the Actions arrow and notice that you can perform tasks such as Download chart details , Download chart image , Field format or set Global visualization preferences . Click on the Global visualization preferences in the Actions menu. Click and explore the tabs for Titles , Tools , Theme . You can launch the Theme Builder to get creative with colors and themes, and save a default.","title":"4. Visualize Data"},{"location":"data-visualization-and-refinery/#conclusion","text":"We've seen a small sampling of the power of Data Refinery on IBM Cloud Pak for Data. We saw how we can transform data using R code, at the command line, or using various Operations on the columns such as changing the data type, removing empty rows, or deleting the column altogether. We next saw that all the steps in our Data Flow are recorded, so we can remove steps, repeat them, or edit an individual step. We were able to quickly profiile the data, so see histograms and statistics for each column. And finally we created more in-depth Visualizations, creating a scatter plot mapping BIRTHDATE vs. TOTALCOST.","title":"Conclusion"},{"location":"machine-learning-autoai/","text":"Automate model building with AutoAI \u00b6 For this part of the workshop, we'll learn how to use AutoAI . AutoAI is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. It automates steps such as preparing your data for modeling, chooses the best algorithm/estimator for your problem, experiments with pipelines and parameters for the trained models. We will use the features of AutoAi to perform an experiment based on the following hypothesis: The TOTALCOST of medications prescribed for a patient is correlated with various features. These features include the following from our data set: *MARITAL,RACE,ETHNICITY,BIRTHPLACE,CITY,STATE,REASONDESCRIPTION,DESCRIPTIONCONDITION* The user persona involved is the Data Scientist. We'll use data from our project in .csv format and then create and configure an AutoAI experiment. If we get a result we like, we can save the model and them promote to our deployment space to be deployed later for consumption via a REST API: This section is broken up into the following steps: Automate model building with AutoAI 1. Run AutoAI Experiment 2. Examine the results 3. Save the model 4. Promote the model Conclusion Note: The lab instructions below assume you have completed the pre-work section already, if not, be sure to complete the pre-work first to create a project and a deployment space. 1. Run AutoAI Experiment \u00b6 Go the (\u2630) menu and click Projects -> All projects and then click on the analytics project that you created in the pre-work section . To start the AutoAI experiment, click the Add to project + button from the top of the page and select the AutoAI experiment option. Name your AutoAI experiment asset and give an optional description, then click the Create button. To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. We will be using one of the CSV file datasets we have preloaded into the project. Click on the Select from project option. In the dialog, select the PATIENTMEDICATIONSCONDITIONS.csv file and click the Select asset button. Once the dataset is loaded, we will need to indicate what we want the model to predict. Under What do you want to predict? panel, select the Prediction column as TOTALCOST . Note that you will use the \"gear icon\" on the Experimental settings button in the next step. AutoAI will set up default values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. Let's make some changes for this experiment. Select the \"gear icon\" on the Experimental settings button. Under the Data source tab on the left toggle the Subsample rows . Our data set has 100K rows, so we can start our experiment with a subset of that to speed up the training. The radio button should be on the default Percent . Use the up/down arrows to change to 10 . Scroll down to Select columns to include . We will uncheck most of these, since for this experiment we don't believe most of the features will be useful. If you performed the data-visualization-and-refinery module, you had a chance to see what the data looked like for each feature (column). Feel free to explore the data to see why we are choosing to include or exclude various feartures. Meanwhile, make sure to uncheck all features except the following: MARITAL,RACE,ETHNICITY,BIRTHPLACE,CITY,STATE,REASONDESCRIPTION,DESCRIPTIONCONDITION . Make sure that it looks like the following screenshots. Next click the Prediction tab on the left, under Experimental settings . You will see that the AutoAI tool has already selected Regression for the prediction type, which makes sense since we are looking at TOTALCOST , a continuous numeric feature. Scroll down to see that you can choose which Algorithms to test and how many top Algorithms to use . We can leave these unchanged. Click on the Runtime tab and examine the settings. We will not change these. Click Save settings . Finally, click Run experiment . The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2): Baseline model (Pipeline 1) Hyperparameter optimization (Pipeline 2) Automated feature engineering (Pipeline 3) Hyperparameter optimization on top of engineered features (Pipeline 4) The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created and evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard. The experiment can take several minutes to complete. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes. 2. Examine the results \u00b6 Once the experiment completes, you can explore the various pipelines and options in the UI. Hover over one of the pipelines in the Relationship map to see the Feature Transformers and Top algorithms used, as well as other details. You can View full log and/or change the Rank by: order. Click on the Pipeline comparison tab to see each pipeline and the associated metrics. Highlight a pipeline for details. Scroll down to see the Pipeline leaderboard . Note that you can rank by various metrics, i.e. Root mean squared error . The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 8 gave the best result for our experiment. Your run of AutoAI might yeild different results. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard. The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. Look at the Feature Transformations tab to see \"new features created during pipeline building, along with the transformation function(s) and the original feature(s) transformed.\" The Feature Importance tab will give the relative weights of the various features in determing the prediction that the model makes. Note that some of the features may be a result of Feature Transformation. What are the results of this experiment? The conclusion I take from this experiment is: There is not a good correlation between the chosen features (MARITAL,RACE,ETHNICITY,BIRTHPLACE,CITY,STATE,REASONDESCRIPTION,DESCRIPTIONCONDITION) and the label we are trying to predict (TOTALCOST). The Root Mean Squared Error in the case of our example is 92,410.338 , which is Terrible ! So, is this experiment a failure? No, it is not a failure as an experiment. Using AutoAi we were able to quickly test a hypothesis and determine that the hypothesis was incorrect. In that sense, this experiment is not a failure. But, we failed to produce a useful Machine Learning model that would enable us to make a good prediction. We could use AutoAI to create further experiments, in the hopes of proving a valid hypothesis and creating a useful machine learning model. Let's have a look at the steps we might take if we had actually created a satisfactory model, and wished to save and deploy that model for use in an application. Close the pipeline details window by clicking on the X in the upper-right corner. 3. Save the model \u00b6 In order to deploy this model, click on the Save as button next to the pipeline you have chosen to save. Choose Model if you want to Create a Watson Machine Learning model asset that you can test with new data, deploy to generate predictions, and trace lineage activity. Choose Notebook if you wish to create a notebook if you want to view the code that created this model pipeline or interact with with the model programatically. Give an optional description and optional tags. Click Create . You will receive a notification to indicate that your model is saved to your project. View the model details by clicking View in project or go back to your project main page by clicking on the project name on the navigator on the top left, and then click the saved model in the Models section of the Assets page. 4. Promote the model \u00b6 In order to deploy the model and utilize a REST endpoint for scoring, click on the Promote to deployment space : Select the deployment space that was created as part of the pre-work as the Target space . Add an optional Description or Tags and click Promote . Note : This is assuming you have already created a deployment space in the pre-work section of the workshop. You will see a notification that the model was promoted to the deployment space succesfully. Using this process, you can deploy a model created in AutoAI for use via a REST endpoint. Conclusion \u00b6 In this module we saw how we can use an AutoAI to test a hypothesis regarding the features and proposed label of a data set. Without spending much time we were able to determine that the hypothesis was not valid, and thus allowed the most efficient use of time for further experiments. The AutoAi framework automatically performed several tasks: Data Wrangling Algorithm Evaluation & Selection Feature Engineering Hyperparameter Optimization In the case where we are satisfied with the results of the experiment, we saw how to save the model and deploy it to the Watson Machine Learning service for scoring of novel data. The choice to save the model as a Jupyter notebook also allows further exploration and manipulation of the various algorithms, pipelines, and hyperparameters.","title":"Machine Learning with AutoAI"},{"location":"machine-learning-autoai/#automate-model-building-with-autoai","text":"For this part of the workshop, we'll learn how to use AutoAI . AutoAI is a capability that automates various tasks to ease the workflow for data scientists that are creating machine learning models. It automates steps such as preparing your data for modeling, chooses the best algorithm/estimator for your problem, experiments with pipelines and parameters for the trained models. We will use the features of AutoAi to perform an experiment based on the following hypothesis: The TOTALCOST of medications prescribed for a patient is correlated with various features. These features include the following from our data set: *MARITAL,RACE,ETHNICITY,BIRTHPLACE,CITY,STATE,REASONDESCRIPTION,DESCRIPTIONCONDITION* The user persona involved is the Data Scientist. We'll use data from our project in .csv format and then create and configure an AutoAI experiment. If we get a result we like, we can save the model and them promote to our deployment space to be deployed later for consumption via a REST API: This section is broken up into the following steps: Automate model building with AutoAI 1. Run AutoAI Experiment 2. Examine the results 3. Save the model 4. Promote the model Conclusion Note: The lab instructions below assume you have completed the pre-work section already, if not, be sure to complete the pre-work first to create a project and a deployment space.","title":"Automate model building with AutoAI"},{"location":"machine-learning-autoai/#1-run-autoai-experiment","text":"Go the (\u2630) menu and click Projects -> All projects and then click on the analytics project that you created in the pre-work section . To start the AutoAI experiment, click the Add to project + button from the top of the page and select the AutoAI experiment option. Name your AutoAI experiment asset and give an optional description, then click the Create button. To configure the experiment, we must first give it the dataset that will be used to train the machine learning model. We will be using one of the CSV file datasets we have preloaded into the project. Click on the Select from project option. In the dialog, select the PATIENTMEDICATIONSCONDITIONS.csv file and click the Select asset button. Once the dataset is loaded, we will need to indicate what we want the model to predict. Under What do you want to predict? panel, select the Prediction column as TOTALCOST . Note that you will use the \"gear icon\" on the Experimental settings button in the next step. AutoAI will set up default values for the experiment based on the dataset and the column selected for the prediction. This includes the type of model to build, the metrics to optimize against, the test/train split, etc. Let's make some changes for this experiment. Select the \"gear icon\" on the Experimental settings button. Under the Data source tab on the left toggle the Subsample rows . Our data set has 100K rows, so we can start our experiment with a subset of that to speed up the training. The radio button should be on the default Percent . Use the up/down arrows to change to 10 . Scroll down to Select columns to include . We will uncheck most of these, since for this experiment we don't believe most of the features will be useful. If you performed the data-visualization-and-refinery module, you had a chance to see what the data looked like for each feature (column). Feel free to explore the data to see why we are choosing to include or exclude various feartures. Meanwhile, make sure to uncheck all features except the following: MARITAL,RACE,ETHNICITY,BIRTHPLACE,CITY,STATE,REASONDESCRIPTION,DESCRIPTIONCONDITION . Make sure that it looks like the following screenshots. Next click the Prediction tab on the left, under Experimental settings . You will see that the AutoAI tool has already selected Regression for the prediction type, which makes sense since we are looking at TOTALCOST , a continuous numeric feature. Scroll down to see that you can choose which Algorithms to test and how many top Algorithms to use . We can leave these unchanged. Click on the Runtime tab and examine the settings. We will not change these. Click Save settings . Finally, click Run experiment . The AutoAI experiment will now run. AutoAI will run through steps to prepare the dataset, split the dataset into training / evaluation groups and then find the best performing algorithms / estimators for the type of model. It will then build the following series of candidate pipelines for each of the top N performing algorithms (where N is a number chosen in the configuration which defaults to 2): Baseline model (Pipeline 1) Hyperparameter optimization (Pipeline 2) Automated feature engineering (Pipeline 3) Hyperparameter optimization on top of engineered features (Pipeline 4) The UI will show progress as different algorithms/evaluators are selected and as different pipelines are created and evaluated. You can view the performance of the pipelines that have completed by expanding each pipeline section in the leaderboard. The experiment can take several minutes to complete. Upon completion you will see a message that the pipelines have been created. Do not proceed to the next section until the experiment completes.","title":"1. Run AutoAI Experiment"},{"location":"machine-learning-autoai/#2-examine-the-results","text":"Once the experiment completes, you can explore the various pipelines and options in the UI. Hover over one of the pipelines in the Relationship map to see the Feature Transformers and Top algorithms used, as well as other details. You can View full log and/or change the Rank by: order. Click on the Pipeline comparison tab to see each pipeline and the associated metrics. Highlight a pipeline for details. Scroll down to see the Pipeline leaderboard . Note that you can rank by various metrics, i.e. Root mean squared error . The next step is to select the model that gives the best result and view its performance. In this case, Pipeline 8 gave the best result for our experiment. Your run of AutoAI might yeild different results. You can view the detailed results by clicking the corresponding pipeline name from the leaderboard. The model evaluation page will show metrics for the experiment, confusion matrix, feature transformations that were performed (if any), which features contribute to the model, and more details of the pipeline. Optionally, feel free to click through these views of the pipeline details. Look at the Feature Transformations tab to see \"new features created during pipeline building, along with the transformation function(s) and the original feature(s) transformed.\" The Feature Importance tab will give the relative weights of the various features in determing the prediction that the model makes. Note that some of the features may be a result of Feature Transformation. What are the results of this experiment? The conclusion I take from this experiment is: There is not a good correlation between the chosen features (MARITAL,RACE,ETHNICITY,BIRTHPLACE,CITY,STATE,REASONDESCRIPTION,DESCRIPTIONCONDITION) and the label we are trying to predict (TOTALCOST). The Root Mean Squared Error in the case of our example is 92,410.338 , which is Terrible ! So, is this experiment a failure? No, it is not a failure as an experiment. Using AutoAi we were able to quickly test a hypothesis and determine that the hypothesis was incorrect. In that sense, this experiment is not a failure. But, we failed to produce a useful Machine Learning model that would enable us to make a good prediction. We could use AutoAI to create further experiments, in the hopes of proving a valid hypothesis and creating a useful machine learning model. Let's have a look at the steps we might take if we had actually created a satisfactory model, and wished to save and deploy that model for use in an application. Close the pipeline details window by clicking on the X in the upper-right corner.","title":"2. Examine the results"},{"location":"machine-learning-autoai/#3-save-the-model","text":"In order to deploy this model, click on the Save as button next to the pipeline you have chosen to save. Choose Model if you want to Create a Watson Machine Learning model asset that you can test with new data, deploy to generate predictions, and trace lineage activity. Choose Notebook if you wish to create a notebook if you want to view the code that created this model pipeline or interact with with the model programatically. Give an optional description and optional tags. Click Create . You will receive a notification to indicate that your model is saved to your project. View the model details by clicking View in project or go back to your project main page by clicking on the project name on the navigator on the top left, and then click the saved model in the Models section of the Assets page.","title":"3. Save the model"},{"location":"machine-learning-autoai/#4-promote-the-model","text":"In order to deploy the model and utilize a REST endpoint for scoring, click on the Promote to deployment space : Select the deployment space that was created as part of the pre-work as the Target space . Add an optional Description or Tags and click Promote . Note : This is assuming you have already created a deployment space in the pre-work section of the workshop. You will see a notification that the model was promoted to the deployment space succesfully. Using this process, you can deploy a model created in AutoAI for use via a REST endpoint.","title":"4. Promote the model"},{"location":"machine-learning-autoai/#conclusion","text":"In this module we saw how we can use an AutoAI to test a hypothesis regarding the features and proposed label of a data set. Without spending much time we were able to determine that the hypothesis was not valid, and thus allowed the most efficient use of time for further experiments. The AutoAi framework automatically performed several tasks: Data Wrangling Algorithm Evaluation & Selection Feature Engineering Hyperparameter Optimization In the case where we are satisfied with the results of the experiment, we saw how to save the model and deploy it to the Watson Machine Learning service for scoring of novel data. The choice to save the model as a Jupyter notebook also allows further exploration and manipulation of the various algorithms, pipelines, and hyperparameters.","title":"Conclusion"},{"location":"openscale-fastpath/","text":"Monitoring models with OpenScale GUI tool using Auto setup \u00b6 This exercise shows a few of the features of the OpenScale GUI tool. When you first provision Watson OpenScale, either in the IBM Cloud or on Cloud Pak for Data, you will be offered the choice to automatically configure and setup OpenScale. This is called the Auto setup, and it walks the admin through the required steps and loads some sample data to demonstrate the features of OpenScale. We will use this automated Auto setup in this lab. It is presumed that OpenScale Auto setup and Watson Machine Learning have already been configured. Use the Insights Dashboard \u00b6 To launch the OpenScale service, go the (\u2630) navigation menu and click Services -> Instances . ![(\u2630) Services -> Instances](../images/navigation/services.png Click the 3 horizontal dots next to the OpenScale instance that your Administrator has provisioned and click Open . Click on the (?) Support tab on the left, and choose `Tour this page. The OpenScale tour will begin. Follow the tour, and when it is done we'll do some exploration of the tools. Now lets interact with the tools. OpenScale will load the Insights Dashboard . This will contain tiles for any models being monitored. The tile for GermanCreditRiskModelICP will be the one we will use for this lab, which was configured using the Auto setup script. Click on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab, and then open the tile for the GermanCreditRiskModelICP model (click the 3-dot menu on the tile and then View Details ): Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score . Click on the triangle with ! under Fairness -> Sex . This indicates that there has been an alert for this attribute in the Fairness monitor. Alerts are configurable, based on thresholds for fairness outcomes which can be set and altered as desired. By moving your mouse pointer over the trend chart, you can see the values change, and which contains bias. Find and click on a spot in the graph that is below the red threshold line to view details. Once you click on one of the time periods, you will see a list of Transactions. Look for one of the Monitored Group - Female with a \"Group Bias\" check mark and Prediction of \"Risk\". Click Explain prediction . If the time period on the graph for Fairness Monitoring doesn't contain such an element, go back and choose another time period until you can find one. This will make the explanation more interesting. Note: Each of the individual transactions can be examined to see them in detail. Doing so will cache that transaction, as we will see later. Be aware of the fact that the Explainability feature requires 1000's of REST calls to the endpoint using variations of the data that are slightly perturbed, which can require several seconds to complete. On the Explain tab for this individual transaction, you can see the relative weights of the most important features for this prediction. Examine the data, then click the Inspect tab. In the Inspect view of this transaction you can see the original features that led to this prediction as well as a series of drop downs and input boxes that offer the ability to change each feature. We can find which features will change the outcome (in this case, from \"Risk\" to \"No Risk\") by clicking the Analysis button. Note that this requires 1000's of REST calls to the endpoint with slight perturbations in the data, so it can take a few minutes. Click the Analysis tab now. In this particular transaction, we see that the presence of a \"guarantor\" on the loan is the only thing required to flip the outcome from \"Risk\" to \"No Risk\". Other transactions might show a different analysis, so please be aware that your results might vary from this. In the case in this example, you can click the drop down for Others on Loan and change to guarantor . Choosing this new value for gurantor will expose a button for Score new values . Click this button. In this example, we can see that the outcome has now been flipped from \"Risk\" to \"No Risk\". Now, go back to the Insights Dashboard page by clicking on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab. This time open the monitor configuration for the GermanCreditRiskModelICP model by clicking the 3-dot menu on the tile and then Configure monitors . Click the Endpoints menu on the left, then the Endpoints tab. Use the Endpoint pulldown to select Debiased transactions . This is the REST endpoint that offers a debiased version of the credit risk ML model, based on the features that were configured (i.e. Sex and Age). It will present an inference that attempts to remove the bias that has been detected. You can see code snippets using cURL, Java, and Python, which can be used in your scripts or applications. Similarly, you can choose the Feedback logging endpoint to get code for Feedback Logging. This provides an endpoint for sending fresh test data for ongoing quality evaluation. You can upload feedback data here or work with your developer to integrate the code snippet provided to publish feedback data to your Watson OpenScale database. Using the Analytics tools \u00b6 Click on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab, and then open the tile for the GermanCreditRiskModelICP model (click the 3-dot menu on the tile and then View Details ): Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score . Click on Analytics -> Predictions by Confidence . It may take a minute or more to create the chart. Here you can see a bar chart that indicates confidence levels and predictions of \"Risk\" and \"No Risk\". From this dashboard click on Analytics -> Chart Builder . Here you can create charts using various Measurements, Features, and Dimensions of your machine learning model. You can see a chart that breaks down Predictions by Confidence Note: You may need to click the date range for 'Past Week' or 'Yesterday' to load the data. You can experiment with changing the values and examine the charts that are created. Conclusion \u00b6 This lab provides a walkthrough of many of the GUI features using the Watson OpenScale tools. The Auto setup deployment creates a machine learning model, deploys it, and inserts historical data to simulate a model that has been used in production over time. The OpenScale monitors are configured, and the user can then explore the various metrics and data. Please continue to explore on your own.","title":"Monitoring models with OpenScale GUI (Auto setup Monitoring)"},{"location":"openscale-fastpath/#monitoring-models-with-openscale-gui-tool-using-auto-setup","text":"This exercise shows a few of the features of the OpenScale GUI tool. When you first provision Watson OpenScale, either in the IBM Cloud or on Cloud Pak for Data, you will be offered the choice to automatically configure and setup OpenScale. This is called the Auto setup, and it walks the admin through the required steps and loads some sample data to demonstrate the features of OpenScale. We will use this automated Auto setup in this lab. It is presumed that OpenScale Auto setup and Watson Machine Learning have already been configured.","title":"Monitoring models with OpenScale GUI tool using Auto setup"},{"location":"openscale-fastpath/#use-the-insights-dashboard","text":"To launch the OpenScale service, go the (\u2630) navigation menu and click Services -> Instances . ![(\u2630) Services -> Instances](../images/navigation/services.png Click the 3 horizontal dots next to the OpenScale instance that your Administrator has provisioned and click Open . Click on the (?) Support tab on the left, and choose `Tour this page. The OpenScale tour will begin. Follow the tour, and when it is done we'll do some exploration of the tools. Now lets interact with the tools. OpenScale will load the Insights Dashboard . This will contain tiles for any models being monitored. The tile for GermanCreditRiskModelICP will be the one we will use for this lab, which was configured using the Auto setup script. Click on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab, and then open the tile for the GermanCreditRiskModelICP model (click the 3-dot menu on the tile and then View Details ): Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score . Click on the triangle with ! under Fairness -> Sex . This indicates that there has been an alert for this attribute in the Fairness monitor. Alerts are configurable, based on thresholds for fairness outcomes which can be set and altered as desired. By moving your mouse pointer over the trend chart, you can see the values change, and which contains bias. Find and click on a spot in the graph that is below the red threshold line to view details. Once you click on one of the time periods, you will see a list of Transactions. Look for one of the Monitored Group - Female with a \"Group Bias\" check mark and Prediction of \"Risk\". Click Explain prediction . If the time period on the graph for Fairness Monitoring doesn't contain such an element, go back and choose another time period until you can find one. This will make the explanation more interesting. Note: Each of the individual transactions can be examined to see them in detail. Doing so will cache that transaction, as we will see later. Be aware of the fact that the Explainability feature requires 1000's of REST calls to the endpoint using variations of the data that are slightly perturbed, which can require several seconds to complete. On the Explain tab for this individual transaction, you can see the relative weights of the most important features for this prediction. Examine the data, then click the Inspect tab. In the Inspect view of this transaction you can see the original features that led to this prediction as well as a series of drop downs and input boxes that offer the ability to change each feature. We can find which features will change the outcome (in this case, from \"Risk\" to \"No Risk\") by clicking the Analysis button. Note that this requires 1000's of REST calls to the endpoint with slight perturbations in the data, so it can take a few minutes. Click the Analysis tab now. In this particular transaction, we see that the presence of a \"guarantor\" on the loan is the only thing required to flip the outcome from \"Risk\" to \"No Risk\". Other transactions might show a different analysis, so please be aware that your results might vary from this. In the case in this example, you can click the drop down for Others on Loan and change to guarantor . Choosing this new value for gurantor will expose a button for Score new values . Click this button. In this example, we can see that the outcome has now been flipped from \"Risk\" to \"No Risk\". Now, go back to the Insights Dashboard page by clicking on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab. This time open the monitor configuration for the GermanCreditRiskModelICP model by clicking the 3-dot menu on the tile and then Configure monitors . Click the Endpoints menu on the left, then the Endpoints tab. Use the Endpoint pulldown to select Debiased transactions . This is the REST endpoint that offers a debiased version of the credit risk ML model, based on the features that were configured (i.e. Sex and Age). It will present an inference that attempts to remove the bias that has been detected. You can see code snippets using cURL, Java, and Python, which can be used in your scripts or applications. Similarly, you can choose the Feedback logging endpoint to get code for Feedback Logging. This provides an endpoint for sending fresh test data for ongoing quality evaluation. You can upload feedback data here or work with your developer to integrate the code snippet provided to publish feedback data to your Watson OpenScale database.","title":"Use the Insights Dashboard"},{"location":"openscale-fastpath/#using-the-analytics-tools","text":"Click on the left-hand menu icon for Insights , make sure that you are on the Model monitors tab, and then open the tile for the GermanCreditRiskModelICP model (click the 3-dot menu on the tile and then View Details ): Notice the red alert indicators on the various monitors (Fairness, Quality, Drift). You should see a red indicator under Fairness. Click on the Fairness score . Click on Analytics -> Predictions by Confidence . It may take a minute or more to create the chart. Here you can see a bar chart that indicates confidence levels and predictions of \"Risk\" and \"No Risk\". From this dashboard click on Analytics -> Chart Builder . Here you can create charts using various Measurements, Features, and Dimensions of your machine learning model. You can see a chart that breaks down Predictions by Confidence Note: You may need to click the date range for 'Past Week' or 'Yesterday' to load the data. You can experiment with changing the values and examine the charts that are created.","title":"Using the Analytics tools"},{"location":"openscale-fastpath/#conclusion","text":"This lab provides a walkthrough of many of the GUI features using the Watson OpenScale tools. The Auto setup deployment creates a machine learning model, deploys it, and inserts historical data to simulate a model that has been used in production over time. The OpenScale monitors are configured, and the user can then explore the various metrics and data. Please continue to explore on your own.","title":"Conclusion"},{"location":"pre-work/","text":"Pre-work \u00b6 This section is broken up into the following steps: Download or Clone the Repository Create an Analytics Project and Deployment Space 1. Download or Clone the Repository \u00b6 Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Once Downloaded, you will need to unzip to extract the files in the repository. Alternately, run the following command: git clone https://github.com/IBM/cpd-workshop-health-care cd cpd-workshop-health-care 2. Create a Project and Deployment Space \u00b6 At this point of the workshop we will be using Cloud Pak for Data for the remaining steps. Log into Cloud Pak for Data \u00b6 Launch a browser and navigate to your Cloud Pak for Data deployment NOTE: Your instructor will provide a URL and credentials to log into Cloud Pak for Data! Create a New project \u00b6 In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc. Go the (\u2630) menu and click Projects -> All projects . Click on New project + . Select Analytics project for the project type and click on Next . We are going to create a project from an existing file (which contains assets we will use throughout this workshop), as opposed to creating an empty project. Select the Create a project from a file option. Navigate to where you cloned this repository, then to projects/ and choose HealthCareProject.zip . Give the project a name and click Create . After succesful creation, click on View new project . You will see the new project with the imported assets. Create a Deployment Space \u00b6 Cloud Pak for Data uses the concept of Deployment Spaces to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc. Go the (\u2630) menu and click Analyze -> Analytics deployments . Click on + New deployment space . Select the Create an empty space option. Give your deployment space a unique name, optional description, then click Create . You will use this space later when you deploy a machine learning model. Next, we will add a collaborator to the new deployment space, so that assets we deploy can be monitored in the OpenScale model monitoring lab. Click on the Access control tab and then click on Add collaborators + on the right. Search for the Admin user, and then click on the Admin choice when it comes up. Click Add to list + and then the Add button. The Admin user will show up under Collaborators . Summary \u00b6 In this pre-work module we have imported the projet for the worshop and created a Deployment Space. We can now move on to the next module.","title":"Pre-work"},{"location":"pre-work/#pre-work","text":"This section is broken up into the following steps: Download or Clone the Repository Create an Analytics Project and Deployment Space","title":"Pre-work"},{"location":"pre-work/#1-download-or-clone-the-repository","text":"Various parts of this workshop will require the attendee to upload files or run scripts that we've stored in the repository. So let's get that done early on, you'll need git on your laptop to clone the repository directly, or access to GitHub.com to download the zip file. To Download, go to the GitHub repo for this workshop and download the archived version of the workshop and extract it on your laptop. Once Downloaded, you will need to unzip to extract the files in the repository. Alternately, run the following command: git clone https://github.com/IBM/cpd-workshop-health-care cd cpd-workshop-health-care","title":"1. Download or Clone the Repository"},{"location":"pre-work/#2-create-a-project-and-deployment-space","text":"At this point of the workshop we will be using Cloud Pak for Data for the remaining steps.","title":"2. Create a Project and Deployment Space"},{"location":"pre-work/#log-into-cloud-pak-for-data","text":"Launch a browser and navigate to your Cloud Pak for Data deployment NOTE: Your instructor will provide a URL and credentials to log into Cloud Pak for Data!","title":"Log into Cloud Pak for Data"},{"location":"pre-work/#create-a-new-project","text":"In Cloud Pak for Data, we use the concept of a project to collect / organize the resources used to achieve a particular goal (resources to build a solution to a problem). Your project resources can include data, collaborators, and analytic assets like notebooks and models, etc. Go the (\u2630) menu and click Projects -> All projects . Click on New project + . Select Analytics project for the project type and click on Next . We are going to create a project from an existing file (which contains assets we will use throughout this workshop), as opposed to creating an empty project. Select the Create a project from a file option. Navigate to where you cloned this repository, then to projects/ and choose HealthCareProject.zip . Give the project a name and click Create . After succesful creation, click on View new project . You will see the new project with the imported assets.","title":"Create a New project"},{"location":"pre-work/#create-a-deployment-space","text":"Cloud Pak for Data uses the concept of Deployment Spaces to configure and manage the deployment of a set of related deployable assets. These assets can be data files, machine learning models, etc. Go the (\u2630) menu and click Analyze -> Analytics deployments . Click on + New deployment space . Select the Create an empty space option. Give your deployment space a unique name, optional description, then click Create . You will use this space later when you deploy a machine learning model. Next, we will add a collaborator to the new deployment space, so that assets we deploy can be monitored in the OpenScale model monitoring lab. Click on the Access control tab and then click on Add collaborators + on the right. Search for the Admin user, and then click on the Admin choice when it comes up. Click Add to list + and then the Add button. The Admin user will show up under Collaborators .","title":"Create a Deployment Space"},{"location":"pre-work/#summary","text":"In this pre-work module we have imported the projet for the worshop and created a Deployment Space. We can now move on to the next module.","title":"Summary"},{"location":"resources/","text":"Additional resources \u00b6 IBM Cloud Pak for Data Platform API IBM Cloud Pak for Data APIs Watson Knowledge Catalog Watson Knowledge Catalog Learning Center IBM Developer Cloud Pak Experiences - Free 7 day trial Cloud Pak for Applications","title":"Additional Resources"},{"location":"resources/#additional-resources","text":"IBM Cloud Pak for Data Platform API IBM Cloud Pak for Data APIs Watson Knowledge Catalog Watson Knowledge Catalog Learning Center IBM Developer Cloud Pak Experiences - Free 7 day trial Cloud Pak for Applications","title":"Additional resources"},{"location":"watson-knowledge-catalog-admin/","text":"Watson Knowledge Catalog for Admins \u00b6 This exercise demonstrates how to solve the problems of enterprise data governance using Watson Knowledge Catalog on the Cloud Pak for Data platform. We'll explain how to use governance, data quality and active policy management in order to help your organization protect and govern sensitive data, trace data lineage and manage data lakes. This knowledge will help users quickly discover, curate, categorize and share data assets, data sets, analytical models and their relationships with other members of your organization. It serves as a single source of truth for data engineers, data stewards, data scientists and business analysts to gain self-service access to data they can trust. You will need the Admin role to create a catalog. This section is comprised of the following steps: Set up Catalog Add Data Assets Add collaborators and control access Add categories Add data classes Add Business terms Add rules for policies 1. Set up Catalog \u00b6 NOTE: The default catalog is your enterprise catalog. It is created automatically after you install the Watson Knowledge Catalog service and is the only catalog to which advanced data curation tools apply. The default catalog is governed so that data protection rules are enforced. The information assets view shows additional properties of the assets in the default catalog to aid curation. Any subsequent catalogs that you create can be governed or ungoverned, do not have an information assets view, and supply basic data curation tools. First we'll create a catalog and load some data Create the catalog \u00b6 Go to the upper-left (\u2630) hamburger menu and choose Organize -> All catalogs . From the Your catalogs page, click the Create catalog button. Give your catalog a name, check the Enforce data protection rules checkbox and provide an optional description. Then click the Create button. Note: Click Ok in the pop up window when selecting the data protection checkbox. 2. Add Data Assets \u00b6 There are several ways to add assets to the catalog. We are going to add a local data asset. There are also optional sections to add connection assets below. Local Data Asset \u00b6 Click Add to Catalog + in the top right and choose Local files . Click the browse link in the 'Select file(s) panel. Browse to the where you cloned the git repository for this workshop or where you downloaded the data assets and choose data/patients.csv file to select it. Add an optional description and click the Add button. NOTE: Stay in the catalog until loading is complete! If you leave the catalog, the incomplete asset will be deleted. The newly added file will show up under the Browse Assets tab of your catalog: (Optional) Add Connection \u00b6 Note: This is an optional part of the lab. You will only be able to add a remote database connection if you have the proper admin credential. You should skip this, unless instructed to add a database connection by the lab instructors. You can add a connection to various data sources, for example DB2 Warehouse in IBM Cloud , by choosing Add to Catalog + -> Connection : Click on the data source type you want to add (for example, Db2 Warehouse ). Enter the connection details and click Create : The connection now shows up in the catalog. Note: Virtualized data can be added to the Default catalog by someone with Administrator or Editor access to that catalog. There is an option to add Data Virtualization as a connection. (Optional) Add Data from Connection \u00b6 Note: This is an optional part of the lab. You should skip this, unless instructed to add virtualized data by the lab instructors. Once you have a connection to a data source, you will be able to add assets from that connection. Click +Add to Catalog -> Connected asset : Click Source -> Select source . Browse under DV to you Schema (i.e. UserXYZW) and choose the joined table. Click Select . A user can now add this to a project like any other asset from a catalog. 2. Add Collaborators and Review Data \u00b6 Under the Access Control tab you can click Add Collaborator to give other users access to your catalog. You can search for a user by entering their name in the Collaborators field. Click on the name to select the user. You can choose a role for the user - Admin , Editor , or Viewer . Then click the Add button. To access data in the catalog, click on the name of the data. A preview of the data will open, with metadata and the first few rows. You can click the Review tab and rate the data, as well as comment on it, to provide feedback to consumers of the data. 3. Add categories \u00b6 The fundamental abstraction in Watson Knowledge Catalog is the Category. A category is analogous to a folder. You can add categories as needed, or you can import them in .csv format. Import categories \u00b6 Note: There are no import files for categories supplied for this lab. This section is just for information only. Import a category for your assets by going to the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Categories , then the click Import button. Click the Add file and navigate to where you cloned/downloaded the workshop repository, choosing data/wkc/glossary-organize-categories.csv . Click the Next button. Under Select merge option choose Replace all values and click Import . You will see \"The import completed succesfully\" when it is completed. In this way, you can import Categories, Business Terms, Classifications, Policies, etc. to populate your governance catalogs. Add category manually \u00b6 In addition to importing, you can manually create categories. Add a category for your assets by going to the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Categories , then click the Create category button on the top right. Give your category a name, such as Personal Data , and an optional description, and then click the Save button. Now, if you hit the Create category link on the Personal Data category screen, you can create a subcategory, such as Residence Information . For the Personal Data category you can select a Type , such as Business term . We can also create classifications for assets, similar to Confidential , Personally Identifiable Information , or Sensitive Personal Information in a similar way, by going to the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Classifications . Click on the New classification button on the top right and then Create new classification from the drop down menu. These classifications can then be added to your category as a Type : 4. Add data classes \u00b6 When you profile your assets, a data class will be inferred from the contents where possible. We'll see more on this later. You can also add your own data classes. Add a data class for your assets by going to the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Data class , then click the New data class button and subsequent Create new data class option from the drop down menu. Give your new data class a name, i.e. alphanumeric , and an optional Primary category and/or description, and click Save as draft . Once the data class is created, we can optionally: add Stewards for this class, and associate classifications and business terms . When you are ready, click the Publish button and again Publish in the pop up window. Now let's add that data class to a column in our patients.csv asset. Go back to the catalog you created earlier (i.e HealthCareCatalog ) and open it ((\u2630) hamburger menu Organize -> All catalogs and choose HealthCareCatalog ). Under the Browse assets tab, click on the data set patients.csv to get the column/row preview. Find the ID column and click the down arrow next to \"Text\" and then View all : In the window that opens, search for your newly created data class, alphanumeric and click it when it returns in the search. Then click the Select button. 5. Add Business terms \u00b6 You can use Business terms to standardize definitions of business concepts so that your data is described in a uniform and easily understood way across your enterprise. You already saw how to create a category and make it a business term . You can also create the business term as it's own entity. From the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Business terms : Click on the upper-right New business term button and then the Create new business term option in the drop down menu. Give the new Business term a name such as Contact Information and optional description ( NOTE: If you are working with others on the same platform, prepend your term with something unique, i.e scottda-ContactInfo ). Click the Save as draft button. A window will come up once the term is created. You can see a rich set of options for creating related terms and adding other metadata. For now, click Publish to make this term available to users of the platform. Go ahead and click Publish on the pop up confirmation window. Go back to the catalog you created earlier (i.e HealthCareCatalog ) and open it ((\u2630) hamburger menu Organize -> All catalogs and choose HealthCareCatalog ). Under the Browse assets tab, click on the data set patients.csv to get the column/row preview. Find the ADDRESS column and click the Column information icon (looks like an \"eye\"). In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms : Enter Contact Information (or your uniquely named term such as scottda-ContactInfo ) term you created earlier under Business terms and the term will be searched for. Click on the Contact Information term that is found, and click Apply : Close that window once the term has been applied. Now, if you wish, do the same thing to add the Contact Information Business term to the CITY , STATE , and ZIP columns. You will now be able to search for these terms from within the platform. For example, going back to your top level HealthCareCatalog , in the search bar with the comment \"What assets are you searching for?\" enter your unique Contact Information term: The patients.csv data set will show up, since it contains columns tagged with the Contact Infomation business term. 6. Add rules for policies \u00b6 We can now create rules to control how a user can access data. Create a business term called PatientID and assign it to your ID column in the data set patients.csv file using the instructions above. See below if you need details, but try it yourself first, and skip to Adding a rule below if you do not need a reminder. How to create a Business term review \u00b6 From the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Business terms . Click on the upper-right + Create Business term button. Give the new Business term the name PatientID and optional description, and click Save as draft . In the next window, click Publish . Now go back to your HealthCareCatalog by opening it up to the column view ((\u2630) hamburger menu Organize -> All catalogs and choose HealthCareCatalog ). Under the Browse assets tab, click on the data set patients.csv to get the column/row preview. Find the ID column and click the Column information icon (looks like an \"eye\"). In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms . Enter PatientID under Business terms and the term will be searched for. Click on the CustumerID term that is found, and click Apply . Then close the pop up window. Adding a rule \u00b6 From the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Rules . Click the New rule button on the top right and then select the Create new rule option from the drop down menu. In the 'Create a new rule' page, select the Data protection rule option. Give your rule a Name , leave the Type set to Access , and add a Business definition . Under Rule builder > Condition1 : For the If condition, select Business term Contains any PatientID . Under Action , for the then panel, select mask data in columns containing alphanumeric . Choose the tile for Substitute , which will make a non-identifiable hash. This obscures the actual ID, but allows actions like database joins to still work. Click the Create rule button. Now if we go back to our patients.csv asset in the catalog at the ID column, it will look the same as before. But a non-admin user will see the \"lock\" icon and see that the customerID has now been substituted with a hash value. To add a rule to Obfuscate data, follow the prior instructions and build a new data protection rule to Obfuscate the Gender And now when that column is viewed by a non-admin user, it will have data that is replaced with similarly formatted data. Conclusion \u00b6 In this lab, we learned how to: Set up Catalog and Data Add collaborators and control access Add categories Add data classes Add Business terms Add rules for policies","title":"Enterprise data governance for Admins using Watson Knowledge Catalog"},{"location":"watson-knowledge-catalog-admin/#watson-knowledge-catalog-for-admins","text":"This exercise demonstrates how to solve the problems of enterprise data governance using Watson Knowledge Catalog on the Cloud Pak for Data platform. We'll explain how to use governance, data quality and active policy management in order to help your organization protect and govern sensitive data, trace data lineage and manage data lakes. This knowledge will help users quickly discover, curate, categorize and share data assets, data sets, analytical models and their relationships with other members of your organization. It serves as a single source of truth for data engineers, data stewards, data scientists and business analysts to gain self-service access to data they can trust. You will need the Admin role to create a catalog. This section is comprised of the following steps: Set up Catalog Add Data Assets Add collaborators and control access Add categories Add data classes Add Business terms Add rules for policies","title":"Watson Knowledge Catalog for Admins"},{"location":"watson-knowledge-catalog-admin/#1-set-up-catalog","text":"NOTE: The default catalog is your enterprise catalog. It is created automatically after you install the Watson Knowledge Catalog service and is the only catalog to which advanced data curation tools apply. The default catalog is governed so that data protection rules are enforced. The information assets view shows additional properties of the assets in the default catalog to aid curation. Any subsequent catalogs that you create can be governed or ungoverned, do not have an information assets view, and supply basic data curation tools. First we'll create a catalog and load some data","title":"1. Set up Catalog"},{"location":"watson-knowledge-catalog-admin/#create-the-catalog","text":"Go to the upper-left (\u2630) hamburger menu and choose Organize -> All catalogs . From the Your catalogs page, click the Create catalog button. Give your catalog a name, check the Enforce data protection rules checkbox and provide an optional description. Then click the Create button. Note: Click Ok in the pop up window when selecting the data protection checkbox.","title":"Create the catalog"},{"location":"watson-knowledge-catalog-admin/#2-add-data-assets","text":"There are several ways to add assets to the catalog. We are going to add a local data asset. There are also optional sections to add connection assets below.","title":"2. Add Data Assets"},{"location":"watson-knowledge-catalog-admin/#local-data-asset","text":"Click Add to Catalog + in the top right and choose Local files . Click the browse link in the 'Select file(s) panel. Browse to the where you cloned the git repository for this workshop or where you downloaded the data assets and choose data/patients.csv file to select it. Add an optional description and click the Add button. NOTE: Stay in the catalog until loading is complete! If you leave the catalog, the incomplete asset will be deleted. The newly added file will show up under the Browse Assets tab of your catalog:","title":"Local Data Asset"},{"location":"watson-knowledge-catalog-admin/#optional-add-connection","text":"Note: This is an optional part of the lab. You will only be able to add a remote database connection if you have the proper admin credential. You should skip this, unless instructed to add a database connection by the lab instructors. You can add a connection to various data sources, for example DB2 Warehouse in IBM Cloud , by choosing Add to Catalog + -> Connection : Click on the data source type you want to add (for example, Db2 Warehouse ). Enter the connection details and click Create : The connection now shows up in the catalog. Note: Virtualized data can be added to the Default catalog by someone with Administrator or Editor access to that catalog. There is an option to add Data Virtualization as a connection.","title":"(Optional) Add Connection"},{"location":"watson-knowledge-catalog-admin/#optional-add-data-from-connection","text":"Note: This is an optional part of the lab. You should skip this, unless instructed to add virtualized data by the lab instructors. Once you have a connection to a data source, you will be able to add assets from that connection. Click +Add to Catalog -> Connected asset : Click Source -> Select source . Browse under DV to you Schema (i.e. UserXYZW) and choose the joined table. Click Select . A user can now add this to a project like any other asset from a catalog.","title":"(Optional) Add Data from Connection"},{"location":"watson-knowledge-catalog-admin/#2-add-collaborators-and-review-data","text":"Under the Access Control tab you can click Add Collaborator to give other users access to your catalog. You can search for a user by entering their name in the Collaborators field. Click on the name to select the user. You can choose a role for the user - Admin , Editor , or Viewer . Then click the Add button. To access data in the catalog, click on the name of the data. A preview of the data will open, with metadata and the first few rows. You can click the Review tab and rate the data, as well as comment on it, to provide feedback to consumers of the data.","title":"2. Add Collaborators and Review Data"},{"location":"watson-knowledge-catalog-admin/#3-add-categories","text":"The fundamental abstraction in Watson Knowledge Catalog is the Category. A category is analogous to a folder. You can add categories as needed, or you can import them in .csv format.","title":"3. Add categories"},{"location":"watson-knowledge-catalog-admin/#import-categories","text":"Note: There are no import files for categories supplied for this lab. This section is just for information only. Import a category for your assets by going to the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Categories , then the click Import button. Click the Add file and navigate to where you cloned/downloaded the workshop repository, choosing data/wkc/glossary-organize-categories.csv . Click the Next button. Under Select merge option choose Replace all values and click Import . You will see \"The import completed succesfully\" when it is completed. In this way, you can import Categories, Business Terms, Classifications, Policies, etc. to populate your governance catalogs.","title":"Import categories"},{"location":"watson-knowledge-catalog-admin/#add-category-manually","text":"In addition to importing, you can manually create categories. Add a category for your assets by going to the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Categories , then click the Create category button on the top right. Give your category a name, such as Personal Data , and an optional description, and then click the Save button. Now, if you hit the Create category link on the Personal Data category screen, you can create a subcategory, such as Residence Information . For the Personal Data category you can select a Type , such as Business term . We can also create classifications for assets, similar to Confidential , Personally Identifiable Information , or Sensitive Personal Information in a similar way, by going to the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Classifications . Click on the New classification button on the top right and then Create new classification from the drop down menu. These classifications can then be added to your category as a Type :","title":"Add category manually"},{"location":"watson-knowledge-catalog-admin/#4-add-data-classes","text":"When you profile your assets, a data class will be inferred from the contents where possible. We'll see more on this later. You can also add your own data classes. Add a data class for your assets by going to the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Data class , then click the New data class button and subsequent Create new data class option from the drop down menu. Give your new data class a name, i.e. alphanumeric , and an optional Primary category and/or description, and click Save as draft . Once the data class is created, we can optionally: add Stewards for this class, and associate classifications and business terms . When you are ready, click the Publish button and again Publish in the pop up window. Now let's add that data class to a column in our patients.csv asset. Go back to the catalog you created earlier (i.e HealthCareCatalog ) and open it ((\u2630) hamburger menu Organize -> All catalogs and choose HealthCareCatalog ). Under the Browse assets tab, click on the data set patients.csv to get the column/row preview. Find the ID column and click the down arrow next to \"Text\" and then View all : In the window that opens, search for your newly created data class, alphanumeric and click it when it returns in the search. Then click the Select button.","title":"4. Add data classes"},{"location":"watson-knowledge-catalog-admin/#5-add-business-terms","text":"You can use Business terms to standardize definitions of business concepts so that your data is described in a uniform and easily understood way across your enterprise. You already saw how to create a category and make it a business term . You can also create the business term as it's own entity. From the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Business terms : Click on the upper-right New business term button and then the Create new business term option in the drop down menu. Give the new Business term a name such as Contact Information and optional description ( NOTE: If you are working with others on the same platform, prepend your term with something unique, i.e scottda-ContactInfo ). Click the Save as draft button. A window will come up once the term is created. You can see a rich set of options for creating related terms and adding other metadata. For now, click Publish to make this term available to users of the platform. Go ahead and click Publish on the pop up confirmation window. Go back to the catalog you created earlier (i.e HealthCareCatalog ) and open it ((\u2630) hamburger menu Organize -> All catalogs and choose HealthCareCatalog ). Under the Browse assets tab, click on the data set patients.csv to get the column/row preview. Find the ADDRESS column and click the Column information icon (looks like an \"eye\"). In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms : Enter Contact Information (or your uniquely named term such as scottda-ContactInfo ) term you created earlier under Business terms and the term will be searched for. Click on the Contact Information term that is found, and click Apply : Close that window once the term has been applied. Now, if you wish, do the same thing to add the Contact Information Business term to the CITY , STATE , and ZIP columns. You will now be able to search for these terms from within the platform. For example, going back to your top level HealthCareCatalog , in the search bar with the comment \"What assets are you searching for?\" enter your unique Contact Information term: The patients.csv data set will show up, since it contains columns tagged with the Contact Infomation business term.","title":"5. Add Business terms"},{"location":"watson-knowledge-catalog-admin/#6-add-rules-for-policies","text":"We can now create rules to control how a user can access data. Create a business term called PatientID and assign it to your ID column in the data set patients.csv file using the instructions above. See below if you need details, but try it yourself first, and skip to Adding a rule below if you do not need a reminder.","title":"6. Add rules for policies"},{"location":"watson-knowledge-catalog-admin/#how-to-create-a-business-term-review","text":"From the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Business terms . Click on the upper-right + Create Business term button. Give the new Business term the name PatientID and optional description, and click Save as draft . In the next window, click Publish . Now go back to your HealthCareCatalog by opening it up to the column view ((\u2630) hamburger menu Organize -> All catalogs and choose HealthCareCatalog ). Under the Browse assets tab, click on the data set patients.csv to get the column/row preview. Find the ID column and click the Column information icon (looks like an \"eye\"). In the window that opens, click the edit icon (looks like a \"pencil\") next to Business terms . Enter PatientID under Business terms and the term will be searched for. Click on the CustumerID term that is found, and click Apply . Then close the pop up window.","title":"How to create a Business term review"},{"location":"watson-knowledge-catalog-admin/#adding-a-rule","text":"From the upper-left (\u2630) hamburger menu, choose Organize -> Data and AI Governance -> Rules . Click the New rule button on the top right and then select the Create new rule option from the drop down menu. In the 'Create a new rule' page, select the Data protection rule option. Give your rule a Name , leave the Type set to Access , and add a Business definition . Under Rule builder > Condition1 : For the If condition, select Business term Contains any PatientID . Under Action , for the then panel, select mask data in columns containing alphanumeric . Choose the tile for Substitute , which will make a non-identifiable hash. This obscures the actual ID, but allows actions like database joins to still work. Click the Create rule button. Now if we go back to our patients.csv asset in the catalog at the ID column, it will look the same as before. But a non-admin user will see the \"lock\" icon and see that the customerID has now been substituted with a hash value. To add a rule to Obfuscate data, follow the prior instructions and build a new data protection rule to Obfuscate the Gender And now when that column is viewed by a non-admin user, it will have data that is replaced with similarly formatted data.","title":"Adding a rule"},{"location":"watson-knowledge-catalog-admin/#conclusion","text":"In this lab, we learned how to: Set up Catalog and Data Add collaborators and control access Add categories Add data classes Add Business terms Add rules for policies","title":"Conclusion"}]}